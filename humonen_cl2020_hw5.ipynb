{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "humonen_cl2020_hw5.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c0ifZe_ocEj"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSkVQm0wocEx",
        "outputId": "902bc2f2-5bdf-4551-dd0c-d3643d6b7d70"
      },
      "source": [
        "!pip install tokenizers\n",
        "from tokenizers import CharBPETokenizer"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.9.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzoeSSrQocEy"
      },
      "source": [
        "Номер 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDsxrMPXocEz"
      },
      "source": [
        "with open('lenta.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw6CPS6kocEz"
      },
      "source": [
        "text = text[:100000]\n",
        "#text"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caSqyLs0ocEz"
      },
      "source": [
        "def freq(text):\n",
        "    frequencies = {}\n",
        "    for i in range(len(text)-1):\n",
        "        current_symbol = text[i]\n",
        "        next_symbol = text[i+1]\n",
        "        pair = current_symbol + next_symbol\n",
        "        if pair in frequencies:\n",
        "            frequencies[pair] += 1\n",
        "        else:\n",
        "            frequencies[pair] = 1\n",
        "    return frequencies"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqkYdAlvocE0"
      },
      "source": [
        "def sort_dict(dictionary, k):\n",
        "    new_dict = {}\n",
        "    keys = list(dictionary.keys())\n",
        "    keys = sorted(keys, key=lambda x: dictionary[x], reverse=True)\n",
        "    keys = keys[:k]\n",
        "    for key in keys:\n",
        "        new_dict[key] = dictionary[key]\n",
        "    return new_dict"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG2veBw9ocE0"
      },
      "source": [
        "def update_text(text, dictionary):\n",
        "    new_text = ()\n",
        "    flag = True\n",
        "    for i in range(len(text)):\n",
        "        if flag:\n",
        "            current_symbol = text[i]\n",
        "            if i < len(text)-1:\n",
        "                next_symbol = text[i+1]\n",
        "            else:\n",
        "                new_text += tuple(current_symbol, )\n",
        "                break\n",
        "            pair = current_symbol + next_symbol\n",
        "            if pair in dictionary: \n",
        "                new_text += (pair, )\n",
        "                flag = False\n",
        "            else:\n",
        "                new_text += (current_symbol, )\n",
        "        else:\n",
        "            flag = True\n",
        "    return new_text"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47cIDCtiocE1"
      },
      "source": [
        "def bpe(text, n, k):\n",
        "    new_text = text\n",
        "    for i in range(n):\n",
        "        frequencies = freq(new_text)\n",
        "        top_freq = sort_dict(frequencies, k)\n",
        "        new_text = update_text(new_text, top_freq)\n",
        "    return new_text"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZvL-LykocE1"
      },
      "source": [
        "Попробуем разные параметры n и k и выведем для них количество токенов и самый длинный токен"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN1_ccjXocE2"
      },
      "source": [
        "def show(result):\n",
        "    print('/'.join(result[:57]))\n",
        "    print('самые частотные токены', Counter(result).most_common(10))\n",
        "    print('самый длинный токен', sorted(result, key=lambda x: len(x), reverse=True)[0])\n",
        "    print('всего токенов', len(result))"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTYCphp0ocE2",
        "outputId": "4e425fea-d68f-4202-9968-95adbb6e236d"
      },
      "source": [
        "for n in range(6, 9, 2):\n",
        "    for k in [100, 200, 450]:\n",
        "        result = bpe(text, n, k)\n",
        "        print('n = ' + str(n), 'k = ' + str(k))\n",
        "        show(result)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n = 6 k = 100\n",
            "Б/о/и /у /С/о/по/ц/ки/на/ и/ Д/ру/ск/ени/к/ за/кон/ч/ил/ис/ь /от/ступ/лени/ем/ г/ер/м/ан/це/в/. Н/е/пр/ия/тель/, пр/и/бли/зи/вш/ис/ь /с/ с/е/ве/ра/ к/ О/со/в/ц/у/ на/ч\n",
            "самые частотные токены [(' ', 525), ('с', 456), ('и', 448), (', ', 418), ('у', 412), ('б', 396), ('на', 386), ('к', 374), ('и ', 368), ('не', 366)]\n",
            "самый длинный токен  сегодня \n",
            "всего токенов 45752\n",
            "n = 6 k = 200\n",
            "Б/о/и /у /С/оп/о/ц/ки/на/ и/ Д/ру/ск/ен/ик/ зак/он/чи/лись /от/ступле/нием/ г/ер/ма/н/це/в. /Н/е/прия/тель/, пр/ибл/из/ив/ши/сь /с /се/ве/ра/ к/ О/со/в/ц/у /нача/л /ар/ти/ллери/йс/ку/ю \n",
            "самые частотные токены [('  ', 291), ('на', 287), (', ', 286), (' в', 281), (' с', 256), ('не', 248), ('и ', 241), ('е ', 226), ('\"', 226), ('по', 218)]\n",
            "самый длинный токен  представител\n",
            "всего токенов 39476\n",
            "n = 6 k = 450\n",
            "Бои/ у/ С/оп/оц/ки/на/ и/ Д/ру/ск/ен/ик/ закон/чи/лись/ отс/туплен/ием /ге/рм/ан/цев./ Н/епри/ятел/ь/, пр/иб/лизи/вш/ись /с /се/ве/ра/ к/ О/со/в/цу /нача/л /артиллер/ийск/ую/ бор/ь/бу/ с/ к/ре/пост/ью/. В /артиллер/ийск\n",
            "самые частотные токены [('  ', 260), (', ', 198), (' в', 164), ('е ', 161), ('не', 160), (' с', 157), ('и ', 153), ('а ', 153), ('в ', 139), ('на', 138)]\n",
            "самый длинный токен Российской Федерации\n",
            "всего токенов 32322\n",
            "n = 8 k = 100\n",
            "Б/о/и /у /С/о/по/ц/ки/на/ и/ Д/ру/ск/ени/к/ за/кон/чил/ис/ь /от/ступ/лени/ем/ г/ер/ман/це/в/. Н/е/прия/тель/, пр/и/бли/зи/вш/ис/ь /с/ с/е/ве/ра/ к/ О/со/в/ц/у/ на/ч/ал/ /арти\n",
            "самые частотные токены [(' ', 429), (', ', 404), ('у', 362), ('с', 358), ('и', 357), ('на', 346), ('не', 341), ('и ', 309), ('в ', 307), ('  ', 303)]\n",
            "самый длинный токен  представител\n",
            "всего токенов 42836\n",
            "n = 8 k = 200\n",
            "Б/о/и /у /С/оп/о/ц/ки/на/ и/ Д/ру/ск/ен/ик/ зак/он/чи/лись /от/ступле/нием/ г/ер/ма/н/це/в. /Н/е/прия/тель/, пр/ибл/из/ив/ши/сь /с /се/ве/ра/ к/ О/со/в/ц/у /нача/л /арти/ллери/йс/ку/ю /бо\n",
            "самые частотные токены [('  ', 274), (', ', 270), (' в', 249), ('на', 241), (' с', 231), ('и ', 222), ('\"', 218), ('по', 203), ('ли', 202), ('не', 200)]\n",
            "самый длинный токен  миллионов долларо\n",
            "всего токенов 36607\n",
            "n = 8 k = 450\n",
            "Бои/ у/ С/оп/оц/ки/на и/ Д/ру/ск/ен/ик/ закон/чи/лись/ отс/туплен/ием /ге/рм/ан/цев./ Н/епри/ятел/ь/, пр/иблизи/вшись /с /се/ве/ра/ к/ О/сов/цу /нача/л /артиллерийск/ую/ борь/бу/ с/ к/ре/пост/ью/. В /артиллерийск/ом/ б/ою /приним/ают /участи/е \n",
            "самые частотные токены [('  ', 242), (', ', 171), ('е ', 140), ('и ', 138), ('не', 127), (' в', 127), (' с', 124), (' и', 121), ('на', 120), ('в ', 111)]\n",
            "самый длинный токен  телерадиокомпании \"Петербург\n",
            "всего токенов 29435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJpBHQU-ocE3"
      },
      "source": [
        "Номер 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t16eB16IocE3"
      },
      "source": [
        "data = pd.read_csv('dataset_ok.csv')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "c5kq9ovkocE3",
        "outputId": "99fb72e5-a65e-4002-b236-47e92aa62e19"
      },
      "source": [
        "data"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>наебалово века, для долбаёбов\\n</td>\n",
              "      <td>INSULT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>вся дума в таком же положении😁\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>а в каком месте массовое столкновение? шрайбик...</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>значит ли это, что контроль за вывозом крупног...</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>вам не нужен щеночек? очень хорошие 🐶🥰\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71982</th>\n",
              "      <td>царствие небесное ,🙏, одноклассник,\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71983</th>\n",
              "      <td>здоровье вам маленькие детки\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71984</th>\n",
              "      <td>я тоже дочери покупала эффекта ноль это развод...</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71985</th>\n",
              "      <td>почему он держит чубайса до сих пор?\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71986</th>\n",
              "      <td>особенно здесь с марий эл работали одни заводы...</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71987 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text   label\n",
              "0                        наебалово века, для долбаёбов\\n  INSULT\n",
              "1                       вся дума в таком же положении😁\\n  NORMAL\n",
              "2      а в каком месте массовое столкновение? шрайбик...  NORMAL\n",
              "3      значит ли это, что контроль за вывозом крупног...  NORMAL\n",
              "4               вам не нужен щеночек? очень хорошие 🐶🥰\\n  NORMAL\n",
              "...                                                  ...     ...\n",
              "71982              царствие небесное ,🙏, одноклассник,\\n  NORMAL\n",
              "71983                     здоровье вам маленькие детки\\n  NORMAL\n",
              "71984  я тоже дочери покупала эффекта ноль это развод...  NORMAL\n",
              "71985             почему он держит чубайса до сих пор?\\n  NORMAL\n",
              "71986  особенно здесь с марий эл работали одни заводы...  NORMAL\n",
              "\n",
              "[71987 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaHAt46Kq83o"
      },
      "source": [
        "data['text'].to_csv('corpus.txt', index=None)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFskhI3JocE3"
      },
      "source": [
        "tok_sub = CharBPETokenizer()\r\n",
        "tok_sub.train('corpus.txt', vocab_size=2000, min_frequency=10,)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "aP05yaEHocE4",
        "outputId": "d329e579-a3e9-4471-e1f7-cb83ccc03634"
      },
      "source": [
        "data['tokenized'] = data['text'].apply(lambda x: tok_sub.encode(x).tokens)\n",
        "data"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>наебалово века, для долбаёбов\\n</td>\n",
              "      <td>INSULT</td>\n",
              "      <td>[на, еба, ло, во&lt;/w&gt;, ве, ка&lt;/w&gt;, ,&lt;/w&gt;, для&lt;/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>вся дума в таком же положении😁\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>[вс, я&lt;/w&gt;, ду, ма&lt;/w&gt;, в&lt;/w&gt;, та, ком&lt;/w&gt;, же...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>а в каком месте массовое столкновение? шрайбик...</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>[а&lt;/w&gt;, в&lt;/w&gt;, ка, ком&lt;/w&gt;, ме, сте&lt;/w&gt;, ма, с...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>значит ли это, что контроль за вывозом крупног...</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>[зна, чи, т&lt;/w&gt;, ли&lt;/w&gt;, это&lt;/w&gt;, ,&lt;/w&gt;, что&lt;/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>вам не нужен щеночек? очень хорошие 🐶🥰\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>[вам&lt;/w&gt;, не&lt;/w&gt;, ну, же, н&lt;/w&gt;, щ, ен, о, че,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71982</th>\n",
              "      <td>царствие небесное ,🙏, одноклассник,\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>[ц, ар, стви, е&lt;/w&gt;, не, бе, с, ное&lt;/w&gt;, ,&lt;/w&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71983</th>\n",
              "      <td>здоровье вам маленькие детки\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>[здоров, ье&lt;/w&gt;, вам&lt;/w&gt;, ма, лен, ь, кие&lt;/w&gt;,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71984</th>\n",
              "      <td>я тоже дочери покупала эффекта ноль это развод...</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>[я&lt;/w&gt;, тоже&lt;/w&gt;, до, че, ри&lt;/w&gt;, по, ку, па, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71985</th>\n",
              "      <td>почему он держит чубайса до сих пор?\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>[почему&lt;/w&gt;, он&lt;/w&gt;, дер, жи, т&lt;/w&gt;, чу, ба, й...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71986</th>\n",
              "      <td>особенно здесь с марий эл работали одни заводы...</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>[о, со, б, ен, но&lt;/w&gt;, з, де, сь&lt;/w&gt;, с&lt;/w&gt;, м...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71987 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  ...                                          tokenized\n",
              "0                        наебалово века, для долбаёбов\\n  ...  [на, еба, ло, во</w>, ве, ка</w>, ,</w>, для</...\n",
              "1                       вся дума в таком же положении😁\\n  ...  [вс, я</w>, ду, ма</w>, в</w>, та, ком</w>, же...\n",
              "2      а в каком месте массовое столкновение? шрайбик...  ...  [а</w>, в</w>, ка, ком</w>, ме, сте</w>, ма, с...\n",
              "3      значит ли это, что контроль за вывозом крупног...  ...  [зна, чи, т</w>, ли</w>, это</w>, ,</w>, что</...\n",
              "4               вам не нужен щеночек? очень хорошие 🐶🥰\\n  ...  [вам</w>, не</w>, ну, же, н</w>, щ, ен, о, че,...\n",
              "...                                                  ...  ...                                                ...\n",
              "71982              царствие небесное ,🙏, одноклассник,\\n  ...  [ц, ар, стви, е</w>, не, бе, с, ное</w>, ,</w>...\n",
              "71983                     здоровье вам маленькие детки\\n  ...  [здоров, ье</w>, вам</w>, ма, лен, ь, кие</w>,...\n",
              "71984  я тоже дочери покупала эффекта ноль это развод...  ...  [я</w>, тоже</w>, до, че, ри</w>, по, ку, па, ...\n",
              "71985             почему он держит чубайса до сих пор?\\n  ...  [почему</w>, он</w>, дер, жи, т</w>, чу, ба, й...\n",
              "71986  особенно здесь с марий эл работали одни заводы...  ...  [о, со, б, ен, но</w>, з, де, сь</w>, с</w>, м...\n",
              "\n",
              "[71987 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN6JwtIqqSpi"
      },
      "source": [
        "#вычислим для каждого слова количество документов, в котором оно встречается\r\n",
        "frqncs = {}\r\n",
        "for document in data['tokenized']:\r\n",
        "  for word in document:\r\n",
        "    if word in frqncs:\r\n",
        "      frqncs[word] += 1\r\n",
        "    else:\r\n",
        "      frqncs[word] = 1"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVaHd9cYr0tO",
        "outputId": "75ef18e1-ab90-47dc-d571-a7ad3c2513e5"
      },
      "source": [
        "doc_len = data.shape[0]\r\n",
        "doc_len"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71987"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yNQoGf6sbcI"
      },
      "source": [
        "#вычислим idf\r\n",
        "idf_dict = {}\r\n",
        "for word in frqncs:\r\n",
        "  idf = np.log(doc_len/frqncs[word])\r\n",
        "  idf_dict[word] = idf"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8LncStdocE4"
      },
      "source": [
        "X = lil_matrix((doc_len, len(idf_dict.keys())))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JtjZp-VocE5"
      },
      "source": [
        "for i in range(len(data['tokenized'])):\r\n",
        "  document = data['tokenized'][i]\r\n",
        "  length = len(document)\r\n",
        "  #вычисляем tf\r\n",
        "  tf_dict = {}\r\n",
        "  for word in document:\r\n",
        "    if word in tf_dict:\r\n",
        "      tf_dict[word] += 1/length\r\n",
        "    else:\r\n",
        "      tf_dict[word] = 1/length\r\n",
        "  #записываем в матрицу (можно конечно было сделать и за один проход)\r\n",
        "  indices = list(idf_dict.keys())\r\n",
        "  for word in document:\r\n",
        "    index = indices.index(word)\r\n",
        "    X[i, index] = tf_dict[word] * idf_dict[word]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71GT9jCE3HWL"
      },
      "source": [
        "y= data['label']"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGg2X03G3MRR"
      },
      "source": [
        "classifier = SGDClassifier(loss=\"log\", max_iter=150)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01OxZ8PrxzKF",
        "outputId": "7e6320bb-e740-42c2-ba0e-90a8870cbe34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cross_val_score(classifier, X, y, scoring=\"f1_micro\")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.87595499, 0.87796916, 0.87761339, 0.8796277 , 0.87879419])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFR4k5YY4XKi"
      },
      "source": [
        ""
      ],
      "execution_count": 92,
      "outputs": []
    }
  ]
}