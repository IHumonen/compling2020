{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "from collections import Counter\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем корпус с опечатками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('data/sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('data/correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
    "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём словарь из правильных словофрм и укороченных правильных словофрм (значение — правильная словоформа) и словарь частотностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#скачиваем корпус(википедию)\n",
    "corpus = open('data/wiki_data.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = Counter(re.findall('\\w+', corpus.lower()))\n",
    "# фунцкия расчета вероятности слова\n",
    "N = sum(WORDS.values())\n",
    "def P(word, N=N): \n",
    "    \"Вычисляем вероятность слова\"\n",
    "    return WORDS[word] / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = set(re.findall('\\w+', corpus.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete(dictionary, word, depth=1): #можно конечно сделать изящнее (рекурсивно), но словарь надо создать один раз\n",
    "    if len(word) > depth:\n",
    "        if depth == 1:\n",
    "                for i in range(len(word)):\n",
    "                    current = word[:i] + word[(i+1):]\n",
    "                    if current not in dictionary:\n",
    "                        dictionary[current] = [word]\n",
    "                    else:\n",
    "                        dictionary[current].append(word)\n",
    "        if depth == 2:\n",
    "            for i in range(len(word)):\n",
    "                for k in range(i+1, len(word)):\n",
    "                    current = word[:i] + word[(i+1):k] + word[k+1:]\n",
    "                    if current not in dictionary:\n",
    "                        dictionary[current] = [word]\n",
    "                    else:\n",
    "                        dictionary[current].append(word)\n",
    "#    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete(delete({}, 'полк'), 'волк')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete(delete({}, 'полк', depth=2), 'волк', depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#проходим по корпусу и добавляем в словарь слова с удаленными буквами\n",
    "vocab_0 = {} #правильные словоформы\n",
    "vocab_1 = {} #удалена 1 буква\n",
    "vocab_2 = {} #удалены 2 буквы\n",
    "for word in corpus:\n",
    "    vocab_0[word] = [word]\n",
    "    delete(vocab_1, word, depth=1)\n",
    "    delete(vocab_2, word, depth=2)\n",
    "vocabs = [vocab_0, vocab_1, vocab_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['волк', 'уолк', 'толк', 'полк', 'фолк', 'иолк']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_1['олк']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, исправляющую опечатки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(c_candidates):\n",
    "    if len(c_candidates) == 0:\n",
    "        return None\n",
    "    choosen = max(c_candidates, key=P) \n",
    "    return choosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vocabs(word, vocabs, candidates, start_weight):\n",
    "    for i in range(len(vocabs)): \n",
    "        vocab = vocabs[i]\n",
    "        if word in vocab:\n",
    "            weight = max(i, start_weight)\n",
    "#            weight  = i + start_weight\n",
    "            candidates[weight] += vocab[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word):\n",
    "#    if word in vocab_0:\n",
    "#        return word\n",
    "    correct = word\n",
    "    candidates = [[], [], [], [], []] \n",
    "    check_vocabs(word, vocabs, candidates, 0)\n",
    "    for i in range(len(word)):\n",
    "        current_1 = word[:i] + word[(i+1):]\n",
    "        check_vocabs(current_1, vocabs, candidates, 1)\n",
    "        for k in range(i+1, len(word)):\n",
    "            current_2 = word[:i] + word[(i+1):k] + word[k+1:]\n",
    "            check_vocabs(current_2, vocabs, candidates, 2)\n",
    "    for c in candidates:\n",
    "        if len(c) > 0:\n",
    "            return choose(c)\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'полк'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "correction('олк')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'бронетранспортёр'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "correction('бронеттанспартёр')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        predicted = correction(pair[1])\n",
    "        \n",
    "        \n",
    "        if predicted == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  predicted:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == predicted:\n",
    "                mistaken_fixed += 1\n",
    "        \n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8579420579420579\n",
      "0.4405218726016884\n",
      "0.07959113357069025\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим триграммную модель: будем подавать в функцию correction не только слово, но и его соседа слева и справа, а вероятность слова в тексте будем домножать на вероятность появления этого слова с такими соседями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    sents = sentenize(text)\n",
    "    return [normalize(sent.text) for sent in sents]\n",
    "\n",
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('data/wiki_data.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wiki = [['<start>'] + sent + ['<end>'] for sent in preprocess(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = {} # словарь контекст : слово : сколько раз слово встречается в этом контексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#собираем триграммы\n",
    "for sent in corpus_wiki:\n",
    "    for i in range(1 , len(sent)-1):\n",
    "        token = sent[i]\n",
    "        context = (sent[i-1], sent[i+1])\n",
    "        if context not in trigrams:\n",
    "            trigrams[context] = {}\n",
    "            trigrams[context][token] = 1\n",
    "        else:\n",
    "            if token not in trigrams[context]:\n",
    "                trigrams[context][token] = 1\n",
    "            else:\n",
    "                trigrams[context][token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
    "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
    "#добавим токены начала и конца предложения\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = ['<start>'] + [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)] + ['<end>']\n",
    "    tokens_2 = ['<start>'] + [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)] + ['<end>']\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('к', 'в')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(trigrams.keys())[101]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "западу 1\n",
      "другу 1\n",
      "постройке 1\n",
      "постановке 1\n",
      "vi 1\n",
      "ней 22\n",
      "возвращению 3\n",
      "участию 23\n",
      "гипоксии 1\n",
      "примеру 14\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word in trigrams[a]:\n",
    "    if i < 10:\n",
    "        print(word, trigrams[a][word])\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавим в функцию выбора вероятность слова в контексте\n",
    "def P_context(word, context):\n",
    "    if context not in trigrams:\n",
    "        return 1 #ничего не знаем про такой контекст \n",
    "    else:\n",
    "        summa = 0\n",
    "        trigram = trigrams[context]\n",
    "        for w in trigram:\n",
    "            summa += trigram[w]\n",
    "        if word in trigram:\n",
    "            return trigram[word]/summa\n",
    "        else:\n",
    "            return 1/(100*N) #слова не встречается в таком контексте, но если мы умножим на ноль вероятность для всех слов, мы не сможем выбрать нужное\n",
    "def choose(c_candidates, context):\n",
    "    if len(c_candidates) == 0:\n",
    "        return None\n",
    "    choosen = max(c_candidates, key=lambda x: P(x) * P_context(x, context)) \n",
    "    return choosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавим контекст в функцию исправления\n",
    "def correction(word, context):\n",
    "#    if word in vocab_0:\n",
    "#        return word\n",
    "    correct = word\n",
    "    candidates = [[], [], [], [], []] \n",
    "    check_vocabs(word, vocabs, candidates, 0)\n",
    "    for i in range(len(word)):\n",
    "        current_1 = word[:i] + word[(i+1):]\n",
    "        check_vocabs(current_1, vocabs, candidates, 1)\n",
    "        for k in range(i+1, len(word)):\n",
    "            current_2 = word[:i] + word[(i+1):k] + word[k+1:]\n",
    "            check_vocabs(current_2, vocabs, candidates, 2)\n",
    "    for c in candidates:\n",
    "        if len(c) > 0:\n",
    "            return choose(c, context)\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим качество новой функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for k in range(1, len(word_pairs)-1):\n",
    "        pair = word_pairs[k]\n",
    "        previous = word_pairs[k-1]\n",
    "        next_ = word_pairs[k+1]\n",
    "        context = (previous[1], next_[1])\n",
    "        predicted = correction(pair[1], context)\n",
    "        \n",
    "        \n",
    "        if predicted == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  predicted:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == predicted:\n",
    "                mistaken_fixed += 1\n",
    "        \n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.859040959040959\n",
      "0.44896392939370683\n",
      "0.07959113357069025\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Было вот так:  \n",
    "0.8579420579420579  \n",
    "0.4405218726016884  \n",
    "0.07959113357069025  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало немного лучше. Попробуем не домножать на вероятность триграммы, а считать её приоритетной "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(c_candidates, context):\n",
    "    if len(c_candidates) == 0:\n",
    "        return None\n",
    "    choosen = max(c_candidates, key=lambda x: (P_context(x, context), P(x))) \n",
    "    return choosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for k in range(1, len(word_pairs)-1):\n",
    "        pair = word_pairs[k]\n",
    "        previous = word_pairs[k-1]\n",
    "        next_ = word_pairs[k+1]\n",
    "        context = (previous[1], next_[1])\n",
    "        predicted = correction(pair[1], context)\n",
    "        \n",
    "        \n",
    "        if predicted == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  predicted:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == predicted:\n",
    "                mistaken_fixed += 1\n",
    "        \n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.859040959040959\n",
      "0.44896392939370683\n",
      "0.07959113357069025\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А так уже ничего не изменилось"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
